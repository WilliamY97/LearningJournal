# Lecture 19 - Memory

## Main Memory

In execution of program, CPU fetches instructions from memory, and decodes the instruction. It may be that the instruction
requires fetching of operands from memory. After the operation is completed, the result may be stored back in memory.

Ex. a single simple instruction like an addition could easily result in four memory accesses.

Executing a program therefore means spending a lot of time interacting with memory.

Like CPU, main memory is a resource that needs to be shared b/w multiple processes. The way programs are written, application developers
behave as if (1) main memory is unlimited, and (2) all of main memory is at the program's disposal.

Simple logic tells us that application developers are wrong: an infinite amount of data storage would req. an infinite amount of space.

One of the major objectives of the operating system is to manage shared resources, and that is exactly what main memory is.

## No Memory Management

Simplest way to manage memory is to not manage it at all. 

Problem: If two programs write to address 1024, then the second program will overwrite the first one's changes and it will probably
result in errors or a crash. If they know of each other, then the first program can use memory locations less than 2048 and the other can use
locations above 2048.

This level of execution gets more and more difficult as more and more programs are introduced to the system and is next to impossible if we
do not control (have the source code to) all the programs that are to execute concurrently.

**Solution**: On every process switch, save the entire contents of memory to disk, and restore
the memory contents of the next process to run.

BUT this kind of swapping is, to say the least, incredibly expensive – imagine swapping out several gigabytes of memory on every process switch – but the problem is avoided because only one process is ever in memory at a time.

- No protection for the operating system. The operating system is typically placed in either low memory (start of address) or high memory (from the end of addresses), or in some cases, a bit of both. An errant memory access might result in overwriting a part of the
OS in memory, which can not only lead to crashes, but could also result in corrupting important files on disk.

** Attempted Fix to Solution **: Keep track of some additional info. Have two values maintained: the
*base* and *limit* addresses. These define the start and end addresses of the program's memory.

Every memory access is then compared to the base address as well as the [base + limit] address.
If an attempted access falls outside the range then it's an error. To make this fast when the
access is happening a possible infinite times - the base and limit variable s are set as registers
and this comparison is done using hardware.

## When is a variable assigned a location in memory? Three obvious times

1. **Compile Time:** If we are certain where the process will be loaded into memory, at compile
time we can convert those variables to address locations. This is what happens in assembly.

2. **Load Time:** At the time when the code is to be loaded into memory, the addresses
are updated. Requires that compiler notifies what numbers are addresses and should be
updated when the program is loaded into memory.

3. **Execution Time:** If programs can move around in memory during execution, then we need
to do the binding at run-time.

## Address Space

- It's clear now that without memory management we get a bunch of problems. So we want
to introduce a layer of abstraction; a layer of indirection. We do this with a concept
called *address space* (a set of addresses that a process can use; each process has its
own address space, independent of other processes' address spaces (except when we create
shared memory).

Instead of altering addresses in memory, we can prefix every memory access with an area
code (like a phone number). The address generated by the CPU is thus the *logical address*
We then add the area code to it to produce the *physical address* (actual location in
memory and the address that it sent over the bus). To speed this up, it is done via some
hardware, and the "area code" is a register called the *relocation register*.

- The process itself doesn't know the physical address; it only knows the log address. This
is a run-time mapping of variables ot memory. We get some protection between processes,
though we would get more protection if we brough back the limit register and compared the
physical address to the base and [base + limit] values again.

This scheme makes it easy to relocate a process in memory by changing the relocation register's
value accordingly. A process that is currently loaded into memory with relocation value 14000
can be easily move to another location.

**Detriment**: Every memory access now includes addition (two if limit register). Comparisons
are quick, but addition is slower because of carry propogation time. So every memory access
has a penalty associated with it to do with the addition of the relocation register value to the
issued CPU address.

## Swapping

To run, a process needs to be in main memory. But we might not be able to keep them all in it.
Processes that are blocked are just taking up space in main memory so we can move them out of
main memory. The process of memory to disk or vice-versa is called *swapping*

Swapping is very painful. If process takes 1 GB of memory we need to swap out 1 GB of memory
to disk. 

Modern OS do not perform this kind of swapping because it is simply too slow. Too much time
would be wasted swapping processes to and from disk. A modified form of swapping is used, but
this is a subject we will return to later on in the examination of memory.

You don't need to put a process back in from disk exactly where it was before. This works because
the relocation register will be updated with the new location of the process when it is moved
back to memory.
