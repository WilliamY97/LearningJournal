#Dynamic Programming Part A

##Fibonacci Numbers

```
FibonacciR(n)
1 if n ≤ 1 then return n
2 else return FibonacciR(n-1) + FibonacciR(n-2)
```
- Straightforward recursive procedure is slow
- We keep calculating the same values over and over... 

##Solution

- We can calculate F(n) in linear time by remembering solutions to the solved problems - **Dynamic Programming**

- Compute solution in a bottom-up fashion

- Trade space for time! 

```
Fibonacci(n)
01 F[0]←0
02 F[1]←1
03 for i ← 2 to n do
04 F[i] ← F[i-1] + F[i-2]
05 return F[n] 
```

History: Dynamic Programming was invented by Richard Bellman in 1950s as a general method for optimizing multi-stage decision process.

##Optimization Problems

- We have to choose one solution out of many - one with the optimal (minimum or maximum) value.
- Store solution to each subproblem in case it should reappear

- **A solution exhibits a structure**: It consists to a string of choices that were made - what choices have to be made to arrive at an optimal solution?

- An algorithm should compute **the optimal value**, plus, if needed, **an optimal solution**

##Multiplying Matrices Example

- Two matrices, **A - n x m matrix** and **B - mxk matrix**, can be multiplied to get **C with dimensions n x k**, using nmk scalar multiplications.

**Problem:** Compute a product of many matrices efficiently

- Matrix problem is associative (AB)C = A(BC)

**Parenthesization matters**

- We need to optimally parenthesize
